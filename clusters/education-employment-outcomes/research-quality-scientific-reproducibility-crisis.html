<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Quality & Scientific Reproducibility Crisis - MacroRoundup</title>
    <link rel="stylesheet" href="../../css/style.css">
</head>
<body>
    <header>
        <div class="container">
            <h1>Research Quality & Scientific Reproducibility Crisis</h1>
            <div class="subtitle">17 articles · Parent: Education & Employment Outcomes</div>
        </div>
    </header>
    
    <div class="container">
        <nav>
            <a href="index.html">← Education & Employment Outcomes</a>
            <a href="../../index.html">Home</a>
        </nav>
        
        <div class="card">
            <h2>Description</h2>
            <p>This subcluster examines systemic problems in academic research quality and scientific integrity that impact economic and educational policy decisions. Articles focus on replication crises, research misconduct, publication bias, and methodological flaws across disciplines. Common sources include peer-reviewed journals like Nature and Science Advances, meta-analyses of research practices, and statistical examinations of publication patterns. Unlike sibling subclusters that analyze educational outcomes or labor market performance directly, this group critiques the underlying research infrastructure that informs those analyses, highlighting how flawed studies, retracted papers, and politicized science compromise evidence-based policymaking in education and employment sectors.</p>
        </div>
        
        <div class="card">
            <h2>Most Representative Articles</h2>
            <ul class="article-list">
                
        <li>
            <div class="title">1. A 12-point checklist for when to doubt a scientific consensus: recent analyses reveal significant issues in the reliability of economic studies, with up to 20% fudged.</div>
            <div class="meta">
                American Enterprise Institute · 2020-09-17
                <span class="similarity">Similarity: 0.81</span>
            </div>
        </li>
        
        <li>
            <div class="title">2. A recent study found 10-20% of results in top American journals may be questionable, raising concerns about the reliability of economic research.</div>
            <div class="meta">
                The Economist · 2016-01-25
                <span class="similarity">Similarity: 0.79</span>
            </div>
        </li>
        
        <li>
            <div class="title">3. Only 39% of psychology papers published in 2015 were replicable, highlighting a significant issue in scientific research reliability.</div>
            <div class="meta">
                The Economist · 2016-02-08
                <span class="similarity">Similarity: 0.75</span>
            </div>
        </li>
        
        <li>
            <div class="title">4. Research in @ScienceAdvances finds papers that fail to replicate receive more citations than those that do replicate, even after publication of replication failures.</div>
            <div class="meta">
                Science · 2021-06-15
                <span class="similarity">Similarity: 0.75</span>
            </div>
        </li>
        
        <li>
            <div class="title">5. A review of nearly 7,000 empirical economics studies found 90% of studies in half of research areas were underpowered, with 80% of remaining studies exaggerating results.</div>
            <div class="meta">
                The Economist · 2020-09-17
                <span class="similarity">Similarity: 0.75</span>
            </div>
        </li>
        
            </ul>
        </div>
        
        <div class="card">
            <h2>Edge Cases (Boundary Articles)</h2>
            <ul class="article-list">
                
        <li>
            <div class="title">1. Diversity in communities correlates with a decline in social capital, impacting civic engagement & trust. Residents in diverse settings are less likely to vote, volunteer, or engage in community projects, with trust levels dropping by nearly 50%.</div>
            <div class="meta">
                Boston Globe · 2015-04-23
                <span class="similarity">Similarity: 0.57</span>
            </div>
            <div class="meta">Almost classified as: P7_S1</div>
            <div class="edge-reason">This article is borderline for the "Research Quality & Scientific Reproducibility Crisis" cluster because while it presents statistical claims about social capital and diversity that could relate to research quality issues, it focuses primarily on sociological findings about community engagement rather than examining problems with academic research methodology, replication failures, or scientific integrity. The article would be better suited for the "Gender & Age Political Preference Gaps" cluster since it directly addresses civic engagement patterns and voting behavior, which are core political participation topics.</div>
        </li>
        
        <li>
            <div class="title">2. The inspection paradox distorts sample data by overrepresenting larger groups, leading to biased averages: e.g., average class sizes appear 90 vs. an actual 35, and prison sentences average 13 years vs. an actual 3.6 years, skewing perceptions.</div>
            <div class="meta">
                Science · 2021-08-19
                <span class="similarity">Similarity: 0.58</span>
            </div>
            <div class="meta">Almost classified as: P10_S1</div>
            <div class="edge-reason">This article is borderline because while it discusses a statistical bias that could contribute to research quality issues, it focuses primarily on a specific sampling methodology problem (the inspection paradox) rather than the broader systemic issues of scientific reproducibility, replication crises, or research misconduct that define the cluster's core theme. The article's emphasis on statistical measurement accuracy makes it more aligned with technical analysis methods than with the institutional and procedural problems affecting scientific integrity.</div>
        </li>
        
        <li>
            <div class="title">3. Researchers who narrowly missed securing grants but persisted in reapplying outperformed those who succeeded initially, achieving 36% more citations & 39% more high-impact papers over a decade.</div>
            <div class="meta">
                The Economist · 2019-06-06
                <span class="similarity">Similarity: 0.58</span>
            </div>
            <div class="meta">Almost classified as: P10_S1</div>
            <div class="edge-reason">This article is borderline because while it touches on research quality metrics (citations and high-impact papers), it's primarily about grant funding outcomes and researcher performance rather than the core issues of scientific reproducibility, replication crises, or research integrity that define the assigned cluster. The content aligns more closely with performance analysis and management evaluation, which explains its higher similarity to the Management & Performance Analysis cluster.</div>
        </li>
        
            </ul>
        </div>
        
        <div class="card">
            <h2>Relationship Dendrogram</h2>
            <div class="dendrogram">
                <img src="../../dendrograms/P10_S11_dendrogram.png" alt="Relationship Dendrogram" onerror="this.style.display='none'">
            </div>
        </div>
        
        <div class="card">
            <h2>Related Subclusters</h2>
            <div class="related-links">
                <a href="#">P6_S1 (0.63)</a><a href="#">P3_S2 (0.61)</a><a href="#">P8_S3 (0.61)</a><a href="#">P8_S1 (0.56)</a><a href="#">P13_S3 (0.56)</a>
            </div>
        </div>
    </div>
    
    <footer>
        MacroRoundup Taxonomy Analysis
    </footer>
</body>
</html>